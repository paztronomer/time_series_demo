{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods to be used:\n",
    "    - Auto Correlation Function\n",
    "    - Smoothing via handcrafted Gaussian Kernel \n",
    "    - Gaussian Process Regression\n",
    "    - Cross Validation \n",
    "    - Lomb Scargle (Fast and Generalized)\n",
    "    - Wavelets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy\n",
    "import pandas as pd\n",
    "import pywt \n",
    "import matplotlib.pyplot as plt\n",
    "# Fro pretty printing\n",
    "import pprint\n",
    "# For lag plot\n",
    "from pandas.plotting import lag_plot\n",
    "# For ACF\n",
    "import statsmodels\n",
    "from statsmodels.tsa.stattools import acf\n",
    "# For zoom-in inside the plot box\n",
    "from mpl_toolkits.axes_grid1.inset_locator import zoomed_inset_axes\n",
    "from mpl_toolkits.axes_grid1.inset_locator import mark_inset\n",
    "# Gridspec\n",
    "import matplotlib.gridspec as gridspec\n",
    "# Vaex for lightweight plotting\n",
    "import vaex as vx\n",
    "# Gaussian Process\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import (RBF, Matern, RationalQuadratic,\n",
    "                                              ExpSineSquared, DotProduct,\n",
    "                                              ConstantKernel, WhiteKernel)\n",
    "# Cross validation\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    import gatspy\n",
    "except:\n",
    "    !{sys.executable} -m pip install gatspy --user\n",
    "    # !conda install --yes --prefix {sys.prefix} gatspy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data, using Kepler light curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import a random Kepler light curve. Time is in days, flux is a relative scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_list = ['DataV_koi_kplr005706966.csv', 'DataV_koi_kplr003348082.csv', 'DataV_koi_kplr003656121.csv',\n",
    "        'DataV_koi_kplr004135665.csv', 'DataV_koi_kplr002302548.csv', 'DataV_koi_kplr006846911.csv',\n",
    "        'DataV_koi_kplr006948054.csv', 'DataV_koi_kplr008030339.csv', 'DataV_koi_kplr008884274.csv', \n",
    "        'DataV_koi_kplr009150870.csv', 'DataV_koi_kplr009153554.csv', 'DataV_koi_kplr011442793.csv',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fname = os.path.join('lc_data', data_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lc_x = pd.read_csv(fname, names=['time', 'flux', 'e_flux'], \n",
    "                   nrows=15000, engine='python')\n",
    "npt_lsst = np.ceil(np.ptp(lc_x.time.values) / 1.6).astype('int')\n",
    "lc_x2 = lc_x[::npt_lsst]\n",
    "lc_x2.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the sampling rate (cadence), we will need it after to scale measurements to day scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cad1 = np.median(np.ediff1d(lc_x['time'].values))\n",
    "cad2 = np.median(np.ediff1d(lc_x2['time'].values))\n",
    "print('Kepler sampling: {0:.2f} d, LSST-like sampling:{1:.2f} d'.format(cad1, cad2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick visualization, both Kepler sample-like and LSST sample-like (best scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(6, 4))\n",
    "ax.scatter(lc_x.time, lc_x.flux, marker='o', s=5, c=lc_x.e_flux, cmap='jet')\n",
    "ax.scatter(lc_x2.time, lc_x2.flux, marker='.', s=50, c='lime', edgecolor='k', lw=0.5)\n",
    "ax.set_xlabel('time d')\n",
    "ax.set_ylabel(r'flux$_{normalized}$')\n",
    "ax.set_ylim([np.min(lc_x.flux) - np.std(lc_x.flux), np.max(lc_x.flux) + np.std(lc_x.flux)])\n",
    "ax.set_title('Same LC with different cadences', color='navy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import light curves into vaex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Whole set\n",
    "ds_all = vx.from_ascii(fname, seperator=\",\", names=['time', 'flux', 'e_flux'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_K = vx.from_pandas(lc_x, name='kplr')\n",
    "ds_L = vx.from_pandas(lc_x2, name='lsst')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "# Set first subplot as active \n",
    "plt.sca(ax[0])\n",
    "ds_all.scatter('time', 'flux', c='k', alpha=0.6, s=0.1, length_limit=len(ds_all))\n",
    "delta_yr = np.ptp(ds_all.minmax('time')) / 365\n",
    "ax[0].set_title(r'Whole LC, $\\Delta$={0:.2f} yr'.format(delta_yr))\n",
    "ax[0].axvline(ds_K.max('time'), color='yellowgreen', lw=2)\n",
    "# Now second plot\n",
    "plt.sca(ax[1])\n",
    "ds_K.scatter('time', 'flux', c='dodgerblue', alpha=0.6, s=1)\n",
    "ds_L.scatter('time', 'flux', c='orange', alpha=1, s=20, edgecolor='k')\n",
    "delta_yr = np.ptp(ds_K.minmax('time')) / 365\n",
    "ax[1].set_title(r'Subsection of $\\Delta$={0:.2f} yr'.format(delta_yr), color='navy')\n",
    "print(ds_K.minmax('time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick checking: lag plot should be random for structures with no memory. The correlation here is a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**This relates to... memory of the system!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.close('all')\n",
    "fig, ax = plt.subplots(1, 2, figsize=(6, 3))\n",
    "lag1 = lag_plot(lc_x.flux, ax=ax[0], marker='.', c='goldenrod', edgecolor='k', lw=0.1)\n",
    "lag2 = lag_plot(lc_x2.flux, ax=ax[1], marker='.', c='dodgerblue', edgecolor='k', lw=0.1)\n",
    "#\n",
    "for sub in ax:\n",
    "    sub.set_aspect('equal')\n",
    "ax[0].set_title('Kepler sampling')\n",
    "ax[1].set_title('LSST-like sampling')\n",
    "plt.subplots_adjust(wspace=0.55)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation Function "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is safe to only look for periods shorter than half the light curve, $k \\leq \\frac{N}{2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the autocorrelation coefficients via statsmodels. Note that the `tsa.stattools.acf` method receives only the flux, thus assuming the spacing between each observation is uniform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acf_coeffs = acf(lc_x.flux.values, unbiased=False, nlags=len(lc_x.flux.values) // 2)\n",
    "tau_k = np.arange(1, acf_coeffs.size + 1, 1)\n",
    "t_d = cad1 * tau_k \n",
    "#\n",
    "print('Number of coefficients from the ACF calculation is: {0}'.format(acf_coeffs.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
    "# Raw results from ACF are 'coarse', because of the nature of the input data\n",
    "ax[0].scatter(tau_k, acf_coeffs, marker='.', s=10, color='navy')\n",
    "# Zoom in\n",
    "if 1:\n",
    "    zoom_factor = 60\n",
    "    ax0_zoom = zoomed_inset_axes(ax[0], zoom_factor, loc=1)\n",
    "    # ax0_zoom.scatter(tau_k, acf_coeffs, marker='.', s=20, color='darkorange')\n",
    "    ax0_zoom.plot(tau_k, acf_coeffs, ',-', lw=2, color='darkorange')\n",
    "    ax0_zoom.set_xlim(3213, 3274)\n",
    "    ax0_zoom.set_ylim(0.375, 0.383)\n",
    "    ## Remove tick labels\n",
    "    ax0_zoom.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "    ax0_zoom.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "    mark_inset(ax[0], ax0_zoom, loc1=2, loc2=4, fc='none', ec='goldenrod')\n",
    "#\n",
    "# Maxima for the coefficients\n",
    "aux_maxima = scipy.signal.argrelextrema(acf_coeffs, np.greater)\n",
    "ax[1].scatter(t_d, acf_coeffs, marker='.', s=10, color='lightgray')\n",
    "ax[1].scatter(t_d[aux_maxima], acf_coeffs[aux_maxima], marker='^', s=20, color='lime', \n",
    "              edgecolor='k', linewidths=0.1)\n",
    "#\n",
    "for axis in ax:\n",
    "    axis.set_ylabel(r'$\\rho$', fontsize=13)\n",
    "ax[0].set_xlabel(r'$\\tau$', fontsize=13)\n",
    "ax[1].set_xlabel(r'time $d$', fontsize=13)\n",
    "#\n",
    "ax[0].set_title('ACF coefficients')\n",
    "ax[1].set_title('Location of local maxima')\n",
    "plt.suptitle('Kepler sampling')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Smooth the ACF coefficient distribution to easyly locate the local maxima. **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian(mu, sigma, x):\n",
    "    return np.exp(np.power(-(x - mu), 2) / (2 * np.power(sigma, 2))) / (sigma * np.sqrt(2 * np.pi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values for the Gaussian (convolution) are empirical, as a compromise between diminish noise and keep the ACF signal. \n",
    "Note we need to trim a bit the result array, due to border padding. Also, remember the normalization, to keep the scale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A suggestion from literature (McQuillan + 2013):\n",
    "```\n",
    "sigma_x = 18 / 2.35\n",
    "x = np.arange(0, 56, 1)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_x = 18 / 2.35\n",
    "x = np.arange(0, 56, 1)\n",
    "acf_g_conv = scipy.signal.convolve(acf_coeffs, gaussian(0, sigma_x, x)) / np.sum(gaussian(0, sigma_x, x))\n",
    "print('Original size of the ACF coefficients array: {0}. The smoothed: {1}'.format(acf_coeffs.size, acf_g_conv.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trim the padded extra section, and re-use the previously defined inteval tau_k\n",
    "Ntrim = acf_g_conv.size - acf_coeffs.size\n",
    "acf_g_conv = acf_g_conv[Ntrim:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Local maxima\n",
    "aux_maxima = scipy.signal.argrelextrema(acf_g_conv, np.greater)\n",
    "# Global maxima\n",
    "idx_gmax = np.argmax(acf_g_conv[aux_maxima])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid\n",
    "fig = plt.figure(figsize=(5, 4))\n",
    "gs1 = gridspec.GridSpec(3, 3)\n",
    "gs1.update(left=0.16, right=0.98, hspace=0.05)\n",
    "ax0 = fig.add_subplot(gs1[: -1, :])\n",
    "ax1 = fig.add_subplot(gs1[-1, :], sharex=ax0)\n",
    "#\n",
    "ax1.scatter(t_d, acf_g_conv - acf_coeffs, marker='*', s=10, color='orange')\n",
    "# \n",
    "ax0.scatter(t_d, acf_g_conv, marker='.', s=10, color='lightgray')\n",
    "ax0.scatter(t_d[aux_maxima], acf_g_conv[aux_maxima], marker='^', s=20, color='lime', \n",
    "              edgecolor='k', linewidths=0.1)\n",
    "# \n",
    "ax0.axvline(t_d[aux_maxima][idx_gmax], lw=2, c='b', alpha=0.5)\n",
    "#\n",
    "ax0.set_ylabel(r'$\\rho$', fontsize=13)\n",
    "ax1.set_xlabel(r'time $d$', fontsize=13)\n",
    "ax1.set_ylabel(r'Gauss - ACF', fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_acf_kplr = t_d[aux_maxima][np.argmax(acf_g_conv[aux_maxima])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Maximum of the ACF: {0:.2f} d'.format(max_acf_kplr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's do the same for the more sparse situation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acf_coeffs_spa = acf(lc_x2.flux.values, unbiased=False, nlags=len(lc_x2.flux.values) // 2)\n",
    "tau_k_spa = np.arange(1, acf_coeffs_spa.size + 1, 1)\n",
    "t_d_spa = cad2 * tau_k_spa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must remember that so far we are assuming an uniform sampling . In the case of Kepler cadence, given the amount of points, for this analysis this is not a concern.\n",
    "\n",
    "In the case of a more sparse time series, changes in the cadence will make us result not be as accurate as if having a regular cadence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].scatter(t_d, acf_coeffs, alpha=0.5, label='ACF for Kepler sampling', c='navy', s=10)\n",
    "ax[0].scatter(t_d_spa, acf_coeffs_spa, label='ACF for downsampled data', c='orange', s=20)\n",
    "# Histogram of the cadence in the data\n",
    "ax[1].hist(np.ediff1d(lc_x2['time'].values), bins=10, histtype='stepfilled', color=['lemonchiffon'], lw=0)\n",
    "ax[1].hist(np.ediff1d(lc_x2['time'].values), bins=10, histtype='step', color=['orange'], lw=2)\n",
    "#\n",
    "ax[0].legend(loc='upper right')\n",
    "ax[0].set_xlabel(r'time $d$', fontsize=13)\n",
    "ax[0].set_ylabel(r'$\\rho$', fontsize=13)\n",
    "ax[1].set_xlabel(r'$x_{(t+1)}-x_{t}$ $d$', fontsize=13)\n",
    "ax[1].set_ylabel('N')\n",
    "ax[0].set_title('ACF for both cadences', color='forestgreen')\n",
    "ax[1].set_title('Histogram of cadence, sparse LC', color='navy')\n",
    "#\n",
    "plt.subplots_adjust(bottom=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Local maxima\n",
    "aux_maxima_spa = scipy.signal.argrelextrema(acf_coeffs_spa, np.greater)\n",
    "# Global maxima\n",
    "idx_gmax = np.argmax(acf_coeffs_spa[aux_maxima_spa])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "max_acf_lsst = t_d_spa[aux_maxima_spa][np.argmax(acf_coeffs_spa[aux_maxima_spa])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = 'Maximum of the ACF, for sparse scenario: {0:.2f} d, this represents a variation'.format(max_acf_lsst)\n",
    "txt +=  ' of {0:.2f}% respect to Kepler-sampling'.format(max_acf_lsst * 100 / max_acf_kplr - 100)\n",
    "print(txt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variation of 17% maybe is within our scientific expectatins, maybe not. Let's try a method to fill gaps, for the case when we miss some observatins, or need more points.\n",
    "Note that a previous knowledge of the expected behaviour increases our chances of get a more accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Gap filling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Fillig the gaps will allow us to have a more regular sampled grid. In the case when fewer observations are available, compared with the main variability length, to have an uniform sampling makes calculations more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: *chains of N >= 4000 and long runs, make your regression more robust. Let it run long enough!*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance of the GP\n",
    "guess_period = max_acf_kplr\n",
    "# length_scale: while larger, the shapes of the samples elongate. Default:1.\n",
    "# length_scale_bounds: lower and upper bounds for lenght_scale. Let's say is a day\n",
    "# periodicity_bounds: lower and upper bounds in periodicity\n",
    "if 0:\n",
    "    kernel = 1.0 * ExpSineSquared(length_scale=guess_period/2., \n",
    "                                  periodicity=guess_period,\n",
    "                                  length_scale_bounds=(guess_period, 1.1*guess_period),\n",
    "                                  periodicity_bounds=(guess_period/2, 1.5 * guess_period),\n",
    "                                 )\n",
    "else:\n",
    "    # Exponential Sine\n",
    "    expsine= 1. * ExpSineSquared(length_scale=guess_period / 2., \n",
    "                                  periodicity=guess_period,\n",
    "                                  length_scale_bounds=(guess_period, 1.1*guess_period),\n",
    "                                  periodicity_bounds=(guess_period/2, 1.5 * guess_period),)\n",
    "    # Radial Basis Function\n",
    "    rbf = 1.0 * RBF(length_scale=2 * guess_period, \n",
    "                    length_scale_bounds=(0.1 * guess_period, 4.0 * guess_period))\n",
    "    # Rational Quadratic\n",
    "    rquad =  1. * RationalQuadratic(length_scale=np.ptp(lc_x2.time.values) / 10, \n",
    "                                    alpha=1)\n",
    "    # Matern (unstable in this scenario)\n",
    "    matern = .5 * Matern(length_scale= 2 * guess_period, \n",
    "                         length_scale_bounds=(0.5 * guess_period, 10 *guess_period), \n",
    "                         nu=1.5)\n",
    "    kernel = expsine + rbf\n",
    "# If want to replicate the result, must use same seed \n",
    "aux_seed = np.random # 45\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=50, random_state=aux_seed)\n",
    "if 1:\n",
    "    # Calculate the prior\n",
    "    x_prior = np.linspace(lc_x2.time.values[0], lc_x2.time.values[-1], 1000)\n",
    "    y_mean_prior, y_std_prior = gp.predict(x_prior[:, np.newaxis], return_std=True)\n",
    "    y_samples_prior = gp.sample_y(x_prior[:, np.newaxis], 1000)\n",
    "\n",
    "# Fit to data using Maximum Likelihood Estimation of the parameters\n",
    "gp.fit(lc_x2.time.values[:, np.newaxis], lc_x2.flux.values)\n",
    "# Posterior\n",
    "x_grid = np.linspace(lc_x2.time.values[0], lc_x2.time.values[-1], 1000)\n",
    "# Make the prediction on the meshed x-axis (ask for MSE as well)\n",
    "y_mean_post, y_std_post = gp.predict(x_grid[:, np.newaxis], return_std=True)\n",
    "y_samples_post = gp.sample_y(x_grid[:, np.newaxis], 5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(8, 4))\n",
    "# Vaex for the Kepler sampling\n",
    "ds_K.scatter('time', 'flux', \n",
    "             label='Kepler sampling (N={0})'.format(len(ds_K)), \n",
    "             c='navy', alpha=0.1, s=10)\n",
    "# Posterior <x>\n",
    "ax.plot(x_grid, y_mean_post, lw=2, c='lime', label='Mean of posterior distribution')\n",
    "# Initial data points\n",
    "ax.plot(lc_x2.time, lc_x2.flux, 'o', c='w', \n",
    "        label='LSST-like data points (N={0})'.format(len(lc_x2.index)), \n",
    "        markersize=4, markeredgecolor='k')\n",
    "#\n",
    "ax.set_title('Gaussian Process Regression result. From 15000 to 72 data points', color='navy')\n",
    "ax.set_xlabel(r'time $d$', fontsize=13)\n",
    "ax.set_ylabel(r'flux$_{normalized}$', fontsize=13)\n",
    "plt.tight_layout()\n",
    "ax.set_facecolor('floralwhite')\n",
    "ax.legend(loc='upper left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = 'Posterior\\n{0}\\nkernel =  {1}'.format('=' * len('Posterior'),gp.kernel_)\n",
    "t += '\\nLog-likelohood ={0:.3f}'.format(gp.log_marginal_likelihood(gp.kernel_.theta))\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note through the iterations the periodicity value was changed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    import pickle\n",
    "    pickle.dump(y_mean_post, open('y_mean_posterior_lsst.pickle', 'wb'))\n",
    "    np.save('y_mean_posterior.npy', y_mean_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Note we trained the Gaussian Process and tested/applied it on the same dataset. In this scenario the model would probably fail to  predict unknown data. Then, we should have different subsets of data on which train and evaluate teh data.\n",
    "\n",
    "What are this sets?\n",
    "- train set: on which the training proceeds\n",
    "- validation set: after the training, evaluation of trained model is made on the validation set\n",
    "- test set: final evaluation of the model, when all looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The problem is... many times we don't have so much data to chunk into pieces. With small *N* we coud easily fall in a strong dependece of the selection of such subsets. Here is when Cross Validation comes to save us: it uses a subsample for training and then evaluate the model on the remaining data, by *folds*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, KFold\n",
    "# 1) Instance the GP\n",
    "# Exponential Sine\n",
    "expsine= 1. * ExpSineSquared(length_scale=guess_period / 2., \n",
    "                              periodicity=guess_period,\n",
    "                              length_scale_bounds=(guess_period, 1.1*guess_period),\n",
    "                              periodicity_bounds=(guess_period/2, 1.5 * guess_period),)\n",
    "# Radial Basis Function\n",
    "rbf = 1.0 * RBF(length_scale=2 * guess_period, \n",
    "                length_scale_bounds=(0.1 * guess_period, 4.0 * guess_period))\n",
    "#\n",
    "kernel = expsine + rbf\n",
    "aux_seed = np.random \n",
    "gaussian_pr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=50, random_state=aux_seed)\n",
    "# 2) Fit the model, using the maximization of the likelihood\n",
    "gaussian_pr.fit(lc_x2.time.values[:, np.newaxis], lc_x2.flux.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define the Cross validation\n",
    "gpr_grid = {\n",
    "    'kernel' : [expsine + rbf],\n",
    "    }\n",
    "n_kfolds = 5\n",
    "CV = GridSearchCV(estimator=GaussianProcessRegressor(), \n",
    "                  param_grid=gpr_grid,\n",
    "                  n_jobs=2, \n",
    "                  cv=n_kfolds,\n",
    "                  refit=True,\n",
    "                  return_train_score=True,\n",
    "                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit\n",
    "CV.fit(lc_x2.time.values[:, np.newaxis], lc_x2.flux.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get the prediction, note the \"phantom dimension\"\n",
    "x_cv = np.linspace(lc_x2.time.values[0], lc_x2.time.values[-1], 1000)[:, None]\n",
    "y_cv = CV.predict(x_cv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9, 6))\n",
    "gs1 = gridspec.GridSpec(3, 3)\n",
    "gs1.update(left=0.14, right=0.9, hspace=0.05)\n",
    "ax0 = fig.add_subplot(gs1[: -1, :])\n",
    "ax1 = fig.add_subplot(gs1[-1, :], sharex=ax0)\n",
    "# Activate the first subplot\n",
    "plt.sca(ax0)\n",
    "# Vaex for the Kepler sampling\n",
    "ds_K.scatter('time', 'flux', label='Kepler sampling (N={0})'.format(len(ds_K)), \n",
    "             c='navy', alpha=0.1, s=10)\n",
    "# CV\n",
    "ax0.plot(x_cv, y_cv, marker='o', color='lime', markersize=1)\n",
    "#\n",
    "# Initial data points\n",
    "ax0.plot(lc_x2.time, lc_x2.flux, 'o', c='w', \n",
    "         label='LSST-like data points (N={0})'.format(len(lc_x2.index)), \n",
    "         markersize=4, markeredgecolor='k')\n",
    "\n",
    "# Only neede d if vaex is going to be used: plt.sca(ax1)\n",
    "# Difference\n",
    "ax1.plot(x_grid, y_mean_post - y_cv, c='blueviolet', marker='*', markersize=1)\n",
    "#\n",
    "ax0.set_title('Gaussian Process Regression and Cross validation', color='green')\n",
    "ax0.set_ylabel(r'flux$_{normalized}$', fontsize=13)\n",
    "plt.tight_layout()\n",
    "ax0.set_facecolor('ghostwhite')\n",
    "ax0.legend(loc='upper left')\n",
    "ax1.set_xlabel(r'time $d$', fontsize=13)\n",
    "ax1.set_ylabel(r'posterior - cv', fontsize=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Results from the CV\n",
    "CV_res = CV.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print the dictionary of the results, but in a nicer way\n",
    "if 0:\n",
    "    pp = pprint.PrettyPrinter(indent=4)\n",
    "    pp.pprint(CV_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now... a different method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Lomb Scargle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different modules available! **gatpsy**, **astropy**, **astroML**... and the less fancy **scipy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Starting with Fast LS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_fls = gatspy.periodic.LombScargleFast(fit_period=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_fls.optimizer.period_range = (1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_fls.fit(lc_x.time.values, lc_x.flux.values, lc_x.e_flux.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First do the search on the coarse grid of frequencies, and then on a second step on a more fine grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fls_period = m_fls.best_period"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the periodogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "periods = np.linspace(1, 100, 1000)\n",
    "scores = m_fls.score(periods)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(6, 4))\n",
    "ax.plot(periods, scores)\n",
    "ax.set_xlabel(r'time $d$')\n",
    "ax.set_ylabel(r'periodogram power')\n",
    "ax.set_title('Periodogram for the Kepler-sampled LC', fontsize=13, color='forestgreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase diagram\n",
    "\n",
    "1) fill an auxiliary array for the predicted shape in the phase diagram. A better fit would be using the first 4 more prominent periods\n",
    "\n",
    "2) chunk time vector to fit in the phase plot\n",
    "\n",
    "3) plot all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "aux_time = np.linspace(0, fls_period, 1000)\n",
    "flux_fit = m_fls.predict(aux_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase = (lc_x.time.values / fls_period) % 1\n",
    "phasefit = (aux_time / fls_period)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ds_K_phase = vx.from_arrays(phase=phase, flux=lc_x.flux.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(7, 5))\n",
    "plt.sca(ax)\n",
    "kw = {\n",
    "    'vmin' : 1,\n",
    "    'colormap' : 'jet',\n",
    "    'background_color' : 'white',\n",
    "}\n",
    "ds_K_phase.plot('phase', 'flux', **kw)\n",
    "# ax[0].errorbar(phase, lc_x.flux.values, lc_x.e_flux.values, fmt='o')\n",
    "ax.plot(phasefit, flux_fit, '-', color='w')\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_title('Phase plot', color='dimgray', fontweight='bold')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to refine the period estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_fls = gatspy.periodic.LombScargleFast(fit_period=True)\n",
    "m2_fls.optimizer.set(period_range=(0.5 * fls_period, 2 * fls_period), first_pass_coverage=10)\n",
    "m2_fls.fit(lc_x.time.values, lc_x.flux.values, lc_x.e_flux.values)\n",
    "aux2_fls_period = m2_fls.best_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Optimized period: {0:.2f} d is {1} times the initial evaluation'.format(aux2_fls_period, \n",
    "                                                                               aux2_fls_period / fls_period))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For noisy light curves, a second optimization would be of special interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's go to the traditional LS, to get the period of the sparse case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the Lomb Scargle estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m_genls4 = gatspy.periodic.LombScargle(Nterms=3, \n",
    "                                       fit_period=True)\n",
    "m_genls1 = gatspy.periodic.LombScargle(Nterms=1, \n",
    "                                       fit_period=True)\n",
    "m_genls4.optimizer.period_range = (1, 100)\n",
    "m_genls1.optimizer.period_range = (1, 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit LS using the observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_genls4.fit(lc_x2.time.values, lc_x2.flux.values)\n",
    "m_genls1.fit(lc_x2.time.values, lc_x2.flux.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get best periods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genls4_periods = m_genls4.find_best_periods()\n",
    "genls1_periods = m_genls1.find_best_periods()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most prominent periods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genls4_periods, genls1_periods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Periodograms, internally calculating the frequency spacing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "genls4_per = m_genls4.periodogram_auto()\n",
    "genls1_per = m_genls1.periodogram_auto()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "ax[0].plot(genls4_per[0], genls4_per[1], ls='-', color='forestgreen')\n",
    "ax[1].plot(genls1_per[0], genls1_per[1], ls='-', color='darkorange')\n",
    "# Mark most prominent periods\n",
    "for p in range(len(genls4_per)):\n",
    "    ax[0].axvline(genls4_periods[p])\n",
    "    ax[1].axvline(genls1_periods[p])\n",
    "# Axis minimal setup\n",
    "for subp in ax:\n",
    "    subp.set_xlim([1, 50])\n",
    "    subp.set_xlabel(r'period $d$', color='navy')\n",
    "    subp.set_ylabel(r'power', color='navy')\n",
    "ax[0].set_title('LS using 4 components for fitting')\n",
    "ax[1].set_title('LS using 1 component for fitting')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's try the above on new datasets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Wavelet Transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "time, flux = lc_x.time.values, lc_x.flux.values\n",
    "s0 = 0.5\n",
    "sn = 2\n",
    "nscales = np.log2(sn / s0)\n",
    "scales = np.arange(s0 / 2, sn, 0.01) \n",
    "m_wv = 'morl' #'morl'#'dmey' #'morl'\n",
    "\n",
    "[cfs, frequencies] = pywt.cwt(flux, \n",
    "                              scales, \n",
    "                              m_wv, )\n",
    "                              #time[1] - time[0])\n",
    "power = np.power(abs(cfs), 2)\n",
    "period = (1. / frequencies ) / cad1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auxiliary vector to sum over each frequency (period)\n",
    "aux_sum = np.sum(power, axis=1)\n",
    "#\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "gs1 = gridspec.GridSpec(3, 5)\n",
    "gs1.update(left=0.14, right=0.98, hspace=0.2, wspace=0.7)\n",
    "ax0 = fig.add_subplot(gs1[: -1, : 3])\n",
    "ax1 = fig.add_subplot(gs1[-1, : 3], sharex=ax0)\n",
    "ax2 = fig.add_subplot(gs1[: -1, 3 :], sharey=ax0)\n",
    "ax0.contourf(time, period, np.log2(power))\n",
    "ax1.plot(time, flux)\n",
    "ax2.plot(aux_sum, np.linspace(period.min(), period.max(), aux_sum.size))\n",
    "#\n",
    "ax0.set_yscale('log')\n",
    "ax0.set_ylim([period.min(), 50])\n",
    "ax0.set_ylabel(r'period $d$')\n",
    "ax1.set_xlabel(r'time $d$')\n",
    "ax0.set_title('Wavelet map and its LC')\n",
    "ax2.set_xlabel(r'sum over period')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
